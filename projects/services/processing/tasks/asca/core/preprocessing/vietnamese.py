"""
Vietnamese text processor for ACSA.
Includes word segmentation, emoji processing, sentiment word normalization, etc.
"""
import re
import unicodedata
import string
from typing import Tuple, Set, Dict, Optional
from pathlib import Path

from .base import TextProcessor


class VietnameseTextProcessor(TextProcessor):
    """Vietnamese text processor with word segmentation."""
    
    VNCORENLP_URL = "https://github.com/vncorenlp/VnCoreNLP/archive/refs/tags/v1.2.zip"
    VNCORENLP_DIR = "VnCoreNLP-1.2"
    VNCORENLP_JAR = "VnCoreNLP-1.2.jar"
    
    def __init__(self, 
                 vncorenlp_path: Optional[str] = None,
                 sentiment_lexicon_path: Optional[str] = None):
        """
        Initialize Vietnamese text processor.
        
        Args:
            vncorenlp_path: Path to VnCoreNLP JAR file
            sentiment_lexicon_path: Path to VietSentiWordnet file
        """
        self.vncorenlp_path = vncorenlp_path
        self.sentiment_lexicon_path = sentiment_lexicon_path
        self._word_segmenter = None
        self._sentiment_lexicons = None
    
    def _download_vncorenlp(self, target_dir: Path) -> Optional[str]:
        """Download and extract VnCoreNLP if not present."""
        import urllib.request
        import zipfile
        import shutil
        
        target_dir.mkdir(parents=True, exist_ok=True)
        zip_path = target_dir / "vncorenlp.zip"
        extract_dir = target_dir / self.VNCORENLP_DIR
        jar_path = extract_dir / self.VNCORENLP_JAR
        
        if jar_path.exists():
            print(f"‚úÖ VnCoreNLP already exists: {jar_path}")
            return str(jar_path)
        
        try:
            print(f"üì• Downloading VnCoreNLP from {self.VNCORENLP_URL}...")
            urllib.request.urlretrieve(self.VNCORENLP_URL, zip_path)
            
            print(f"üì¶ Extracting to {target_dir}...")
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(target_dir)
            
            # The zip contains VnCoreNLP-1.2/VnCoreNLP-1.2/...
            inner_dir = target_dir / f"VnCoreNLP-{self.VNCORENLP_DIR.split('-')[1]}"
            if inner_dir.exists() and not extract_dir.exists():
                shutil.move(str(inner_dir), str(extract_dir))
            
            # Clean up
            zip_path.unlink()
            
            if jar_path.exists():
                print(f"‚úÖ VnCoreNLP downloaded successfully: {jar_path}")
                return str(jar_path)
            else:
                print(f"‚ö†Ô∏è JAR not found after extraction at {jar_path}")
                return None
                
        except Exception as e:
            print(f"‚ùå Failed to download VnCoreNLP: {e}")
            return None
        
    @property
    def word_segmenter(self):
        """Lazy load word segmenter (pyvi fallback)."""
        if self._word_segmenter is None:
            vncorenlp_path = self.vncorenlp_path
            
            # Try to find or download VnCoreNLP
            if not vncorenlp_path:
                # Check common locations
                possible_paths = [
                    Path("/opt/airflow/VnCoreNLP-1.2/VnCoreNLP-1.2.jar"),
                    Path.home() / "VnCoreNLP-1.2" / "VnCoreNLP-1.2.jar",
                    Path(__file__).parent.parent / "VnCoreNLP-1.2" / "VnCoreNLP-1.2.jar",
                ]
                for p in possible_paths:
                    if p.exists():
                        vncorenlp_path = str(p)
                        break
            
            if vncorenlp_path:
                try:
                    from vncorenlp import VnCoreNLP
                    self._word_segmenter = VnCoreNLP(
                        vncorenlp_path,
                        annotators="wseg",
                        quiet=True
                    )
                    print(f"‚úÖ VnCoreNLP loaded from: {vncorenlp_path}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Could not load VnCoreNLP: {e}")
                    print("Falling back to pyvi tokenizer")
                    self._word_segmenter = "pyvi"
            else:
                print("‚ÑπÔ∏è VnCoreNLP not found, using pyvi tokenizer")
                self._word_segmenter = "pyvi"
                
        return self._word_segmenter
    
    @property
    def sentiment_lexicons(self) -> Tuple[Set, Set, Set, Dict]:
        """Lazy load sentiment lexicons."""
        if self._sentiment_lexicons is None:
            self._sentiment_lexicons = self._load_sentiment_lexicon()
        return self._sentiment_lexicons
    
    def lowercase(self, text: str) -> str:
        """Convert text to lowercase."""
        return text.lower()
    
    def remove_elongated_chars(self, text: str) -> str:
        """Remove elongated characters (e.g., ƒë·∫πppppp -> ƒë·∫πp)."""
        pattern = rf"(\w)\1+"
        text = re.sub(pattern, r'\1', text)
        return text
    
    def normalize_unicode(self, text: str) -> str:
        """Normalize Unicode characters."""
        return unicodedata.normalize("NFC", text)
    
    def process_emojis(self, text: str) -> str:
        """Convert emojis to sentiment labels."""
        emojis_list = {
            "üëπ": "negative", "üëª": "positive", "üíÉ": "positive", 'ü§ô': 'positive ',
            'üëç': 'positive ', "üíÑ": "positive", "üíé": "positive", "üí©": "positive",
            "üòï": "negative", "üò±": "negative", "üò∏": "positive", "üòæ": "negative",
            "üö´": "negative", "ü§¨": "negative", "üßö": "positive", "üß°": "positive",
            'üê∂': 'positive ', 'üëé': 'negative ', 'üò£': 'negative ', '‚ú®': 'positive ',
            '‚ù£': 'positive ', '‚òÄ': 'positive ', '‚ô•': 'positive ', 'ü§©': 'positive ',
            'like': 'positive ', ':))': 'positive ', ':)': 'positive ',
            'he he': 'positive ', 'hehe': 'positive ', 'hihi': 'positive ',
            'haha': 'positive ', 'hjhj': 'positive ', ' lol ': 'negative ',
            ' cc ': 'negative ', 'huhu': 'negative ', '><': 'positive ',
            'üíå': 'positive ', 'ü•∞': 'positive ', 'üôÜ': 'positive ', 'üòÖ': 'negative ',
            'ü§í': 'negative ', 'ü§®': 'negative ', 'ü§¶': 'negative ', 'üò¨': 'negative ',
            'üîã': 'positive ', 'üíî': 'negative ', 'ü§Æ': 'negative ', '‚úã': 'positive ',
            'ü§£': 'positive ', 'üñ§': 'positive ', 'ü§§': 'positive ', ':(': 'negative ',
            'üò¢': 'negative ', '‚ù§': 'positive ', 'üòç': 'positive ', 'üòò': 'positive ',
            'üò™': 'negative ', 'üòä': 'positive ', '?': ' ? ', 'üòÅ': 'positive ',
            'üíñ': 'positive ', 'üòü': 'negative ', 'üò≠': 'negative ', 'üíØ': 'positive ',
            'üíó': 'positive ', '‚ô°': 'positive ', 'üíú': 'positive ', 'ü§ó': 'positive ',
            '^^': 'positive ', 'üò®': 'negative ', '‚ò∫': 'positive ', 'üíã': 'positive ',
            'üëå': 'positive ', 'üòñ': 'negative ', 'üòÄ': 'positive ', ':((': 'negative ',
            'üò°': 'negative ', 'üò†': 'negative ', 'üòí': 'negative ', 'üôÇ': 'positive ',
            'üòè': 'negative ', 'üòù': 'positive ', 'üòÑ': 'positive ', 'üòô': 'positive ',
            'üò§': 'negative ', 'üòé': 'positive ', 'üòÜ': 'positive ', 'üíö': 'positive ',
            '‚úå': 'positive ', 'üíï': 'positive ', 'üòû': 'negative ', 'üòì': 'negative ',
            'Ô∏èüÜóÔ∏è': 'positive ', 'üòâ': 'positive ', 'üòÇ': 'positive ', ':v': 'positive ',
            '=))': 'positive ', 'üòã': 'positive ', 'üíì': 'positive ', 'üòê': 'negative ',
            ':3': 'positive ', 'üò´': 'negative ', 'üò•': 'negative ', 'üòÉ': 'positive ',
            'üòå': ' üòå ', 'üíõ': 'positive ', 'ü§ù': 'positive ', 'üéà': 'positive ',
            'üòó': 'positive ', 'ü§î': 'negative ', 'üòë': 'negative ', 'üî•': 'negative ',
            'üôè': 'negative ', 'üÜó': 'positive ', 'üòª': 'positive ', 'üíô': 'positive ',
            'üíü': 'positive ', 'üòö': 'positive ', '‚ùå': 'negative ', 'üëè': 'positive ',
            ';)': 'positive ', '<3': 'positive ', 'üåù': 'positive ', 'üå∑': 'positive ',
            'üå∏': 'positive ', 'üå∫': 'positive ', 'üåº': 'positive ', 'üçì': 'positive ',
            'üêÖ': 'positive ', 'üêæ': 'positive ', 'üëâ': 'positive ', 'üíê': 'positive ',
            'üíû': 'positive ', 'üí•': 'positive ', 'üí™': 'positive ', 'üí∞': 'positive ',
            'üòá': 'positive ', 'üòõ': 'positive ', 'üòú': 'positive ', 'üôÉ': 'negative ',
            'ü§ë': 'positive ', 'ü§™': 'positive ', '‚òπ': 'negative ', 'üíÄ': 'negative ',
            'üòî': 'negative ', 'üòß': 'negative ', 'üò©': 'negative ', 'üò∞': 'negative ',
            'üò≥': 'negative ', 'üòµ': 'negative ', 'üò∂': 'negative ', 'üôÅ': 'negative ',
            'üéâ': 'positive '
        }
        for emoji, label in emojis_list.items():
            text = text.replace(emoji, f'EMO{label.upper()}')
        text = ' '.join(text.split())
        return text
    
    def normalize_sentiment_words(self, text: str) -> str:
        """Normalize common Vietnamese sentiment words and abbreviations."""
        sentiment_word_map = {
            '√¥ k√™i': ' ok ', 'okie': ' ok ', ' o k√™ ': ' ok ',
            'okey': ' ok ', '√¥k√™': ' ok ', 'oki': ' ok ', ' oke ': ' ok ',
            ' okay': ' ok ', 'ok√™': ' ok ',
            ' tks ': ' c√°m ∆°n ', 'thks': ' c√°m ∆°n ', 'thanks': ' c√°m ∆°n ',
            'ths': ' c√°m ∆°n ', 'thank': ' c√°m ∆°n ',
            '‚≠ê': 'star ', '*': 'star ', 'üåü': 'star ',
            'kg ': ' kh√¥ng ', 'not': ' kh√¥ng ', ' kg ': ' kh√¥ng ',
            '"k ': ' kh√¥ng ', ' kh ': ' kh√¥ng ', 'k√¥': ' kh√¥ng ',
            'hok': ' kh√¥ng ', ' kp ': ' kh√¥ng ph·∫£i ', ' k√¥ ': ' kh√¥ng ',
            '"ko ': ' kh√¥ng ', ' ko ': ' kh√¥ng ', ' k ': ' kh√¥ng ',
            'khong': ' kh√¥ng ', ' hok ': ' kh√¥ng ',
            'cute': ' d·ªÖ th∆∞∆°ng ', ' vs ': ' v·ªõi ', 'wa': ' qu√° ',
            'w√°': ' qu√°', 'j': ' g√¨ ', '"': ' ',
            ' sz ': ' c·ª° ', 'size': ' c·ª° ', ' ƒëx ': ' ƒë∆∞·ª£c ',
            'dk': ' ƒë∆∞·ª£c ', 'dc': ' ƒë∆∞·ª£c ', 'ƒëk': ' ƒë∆∞·ª£c ', 'ƒëc': ' ƒë∆∞·ª£c ',
            'authentic': ' chu·∫©n ch√≠nh h√£ng ', ' aut ': ' chu·∫©n ch√≠nh h√£ng ',
            ' auth ': ' chu·∫©n ch√≠nh h√£ng ', 'store': ' c·ª≠a h√†ng ',
            'shop': ' c·ª≠a h√†ng ', 'sp': ' s·∫£n ph·∫©m ', 'gud': ' t·ªët ',
            'god': ' t·ªët ', 'wel done': ' t·ªët ', 'good': ' t·ªët ',
            'g√∫t': ' t·ªët ', 's·∫•u': ' x·∫•u ', 'gut': ' t·ªët ', ' tot ': ' t·ªët ',
            ' nice ': ' t·ªët ', 'perfect': 'r·∫•t t·ªët', 'bt': ' b√¨nh th∆∞·ªùng ',
            'time': ' th·ªùi gian ', 'q√°': ' qu√° ', ' ship ': ' giao h√†ng ',
            ' m ': ' m√¨nh ', ' mik ': ' m√¨nh ', '·ªÉ': '·ªÉ',
            'product': 's·∫£n ph·∫©m', 'quality': 'ch·∫•t l∆∞·ª£ng', 'chat': ' ch·∫•t ',
            'excelent': 'ho√†n h·∫£o', 'bad': 't·ªá', 'fresh': ' t∆∞∆°i ', 'sad': ' t·ªá ',
            'date': ' h·∫°n s·ª≠ d·ª•ng ', 'hsd': ' h·∫°n s·ª≠ d·ª•ng ',
            'quickly': ' nhanh ', 'quick': ' nhanh ', 'fast': ' nhanh ',
            'delivery': ' giao h√†ng ', ' s√≠p ': ' giao h√†ng ',
            'beautiful': ' ƒë·∫πp tuy·ªát v·ªùi ', ' tl ': ' tr·∫£ l·ªùi ', ' r ': ' r·ªìi ',
            ' shopE ': ' c·ª≠a h√†ng ', ' order ': ' ƒë·∫∑t h√†ng ',
            'ch·∫•t lg': ' ch·∫•t l∆∞·ª£ng ', ' sd ': ' s·ª≠ d·ª•ng ', ' dt ': ' ƒëi·ªán tho·∫°i ',
            ' nt ': ' nh·∫Øn tin ', 's√†i': ' x√†i ', 'bjo': ' bao gi·ªù ',
            'thick': ' th√≠ch ', 'thik': ' th√≠ch ', ' sop ': ' c·ª≠a h√†ng ',
            ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': ' r·∫•t ',
            'qu·∫£ ng ': ' qu·∫£ng  ', 'dep': ' ƒë·∫πp ', ' xau ': ' x·∫•u ',
            'delicious': ' ngon ', 'h√†g': ' h√†ng ', 'q·ªßa': ' qu·∫£ ',
            'iu': ' y√™u ', 'fake': ' gi·∫£ m·∫°o ', 'trl': 'tr·∫£ l·ªùi',
            ' por ': ' t·ªá ', ' poor ': ' t·ªá ', 'ib': ' nh·∫Øn tin ',
            'rep': ' tr·∫£ l·ªùi ', 'fback': ' feedback ', 'fedback': ' feedback '
        }
        for word, replacement in sentiment_word_map.items():
            text = text.replace(word, replacement)
        return text
    
    def remove_punctuation(self, text: str) -> str:
        """Remove punctuation from text."""
        translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))
        return text.translate(translator)
    
    def _load_sentiment_lexicon(self) -> Tuple[Set, Set, Set, Dict]:
        """Load sentiment lexicon from VietSentiWordnet file or default sets."""
        not_words = {
            "kh√¥ng", 'kh√¥ng_h·ªÅ', "ch·∫≥ng", "ch∆∞a", "kh√¥ng_ph·∫£i", "ch·∫£", "m·∫•t",
            "thi·∫øu", "v√¥", "ƒë·∫øch", "ƒë√©o", "k√©m", "n·ªè", "not",
            "b·ªõt", "kh√¥ng_bao_gi·ªù",
        }
        
        positive_words = {
            "∆∞ng_√Ω", "∆∞ng", "k·ªπ", "ƒë∆∞·ª£c", "√¥_k√™", "ok", "m·ªãn", "·ªïn", "xinh",
            "ch√∫c_m·ª´ng", "h·∫°nh_ph√∫c", "sang", "o√°ch", "khen", "·ªïn_ƒë·ªãnh",
            "c·∫£m_∆°n", "c√°m_∆°n", "chu·∫©n", "ho√†n_thi·ªán", "ch·∫Øc_ch·∫Øn", "s·∫°ch_s·∫Ω",
            "h√†i_l√≤ng", "ch·∫•t_l∆∞·ª£ng", "h·∫•p_d·∫´n", "vui_v·∫ª", "nguy√™n_ch·∫•t",
            "thu·∫≠n_l·ª£i", "c√≥_l·ª£i", "t√≠ch_c·ª±c", "khuy·∫øn_kh√≠ch", "t·ªët_h∆°n",
            "v·ªã_tha", "s·∫Øc", "b√©n", "th√≠ch_h·ª£p", "qu√Ω_b√°u", "s√¢u_s·∫Øc",
            "th·ªãnh_v∆∞·ª£ng", "xinh_ƒë·∫πp", "r·ª±c_r·ª°", "trong_s√°ng", "ch·∫•p_nh·∫≠n_ƒë∆∞·ª£c",
            "kh√©o_l√©o", "ngh·ªá_thu·∫≠t", "y√™n_t√¢m", "uy·ªÉn_chuy·ªÉn", "s√¥i_ƒë·ªông",
            "nh√¢n_ƒë·∫°o", "th√¢n_m·∫≠t", "tho·∫£i_m√°i", "ƒë·∫∑c_bi·ªát", "to√†n_di·ªán",
            "h√≤a_ƒë·ªìng", "h√†i_h√≤a", "thu·∫≠n_ti·ªán", "l·ªãch_s·ª±", "may_m·∫Øn", "may",
            "ƒëoan_trang", "ph·∫•n_ch·∫•n", "s√†nh_ƒëi·ªáu", "s√°ng_su·ªët", "k√≠n_ƒë√°o",
            "m√°t_m·∫ª", "l·∫•p_l√°nh", "danh_d·ª±", "d·ªÖ_d√†ng", "say_m√™", "nhi·ªát_t√¨nh",
            "ƒë·∫°o_ƒë·ª©c", "trung_th·ª±c", "trung_th√†nh", "chung_th·ªßy", "ngon",
            "chu_ƒë√°o", "ngƒÉn_n·∫Øp", "l√†nh_m·∫°nh", "h·ª£p_v·ªá_sinh", "kh√¥n",
            "khen_ng·ª£i", "qu√Ω_gi√°", "kh√°ng_khu·∫©n", "√™m_tai", "tinh_t√∫y",
            "du_d∆∞∆°ng", "b·ªï_√≠ch", "h·ªìng_h√†o", "kh·ªèe_kho·∫Øn", "kh·ªèe_m·∫°nh",
            "kh·ªèe", "m·∫°nh", "sƒÉn_ch·∫Øc", "sung_s·ª©c", "m·∫°nh_kh·ªèe", "tr·∫ª_trung",
            "ƒë√πa", "ƒë·ªÅ_cao", "qu·∫£n_l√Ω", "c√°nh_tay_ph·∫£i", "nh·∫≠n_d·∫°ng_ƒë∆∞·ª£c",
            "ho√†n_h·∫£o", "tr·ªçn_v·∫πn", "l√Ω_t∆∞·ªüng", "d·ªÖ_an_·ªßi", "ƒë·∫πp", "duy√™n_d√°ng",
            "tuy·ªát_v·ªùi", "ƒë√°ng_ng∆∞·ª°ng_m·ªô", "th√∫_v·ªã", "ng·ªçt_ng√†o", "l·∫°c_quan",
            "sinh_l·ª£i", "ch√≠nh_ƒë√°ng", "khi√™m_t·ªën", "minh_m·∫´n", "uy_t√≠n",
            "vinh_d·ª±", "th·∫≥ng_th·∫Øn", "b·∫£o_ƒë·∫£m", "m√†u_m·ª°", "d·ªÖ_ch·ªãu", "t∆∞∆°i",
            "c·∫©n_th·∫≠n", "ƒë√∫ng", "hi·ªáu_qu·∫£", "cute", "d·ªÖ_th∆∞∆°ng", "ph√™", "x·ªãn",
            "s·ªãn", "vui_t√≠nh", "ch√≠nh_h√£ng", "th·ª±c_s·ª±", "vinh_quang",
            "th√°nh_thi·ªán", "vui_t∆∞∆°i", "g·ª£i_c·∫£m", "c√¢n_ƒë·ªëi", "ch√¢n_th√†nh",
            "th√†nh_th·∫°o", "tinh_t·∫ø", "ki√™n_c·ªë", "th√¢n_thi·ªán", "th√≠ch",
            "t·ªèa_s√°ng", "ng∆∞·ª°ng_m·ªô", "ph√π_h·ª£p", "hy_v·ªçng", "t·ªët_ƒë·∫πp", "t·ªët",
            "gi·ªèi_giang", "l√¥i_cu·ªën", "uy√™n_b√°c", "y√™u", "th√≠ch_th√∫", "√°i_√¢n",
            "ch√¢n_t√¨nh", "chƒÉm_ch√∫t", "tuy·ªát", "nh·∫π_nh√µm", "xinh_x·∫Øn", "gi·ªèi",
            "kh·ªßng", "ƒë·∫°t", "h·ª£p_l√Ω", "h·ª£p_l√≠", "s·∫°ch", "·∫•m", "m·ªÅm",
            "c·∫£i_thi·ªán", "ti·ªán", "g·ªçn", "tin_t∆∞·ªüng", "nh·∫°y", "nh·∫°y_b√©n",
            "pin_r·∫•t_tr√¢u", "bao_m∆∞·ª£t", "pin_tr√¢u", "s·∫°c_nhanh"
        }
        
        negative_words = {
            "b·∫•t_l·ª£i", "ch√°n", "ch·∫≠t_h·∫πp", "ch·∫≠t", "t·ª©c_gi·∫≠n", "x·∫•u",
            "kh·ªßng_khi·∫øp", "m·ªèng", "nh·∫ßm", "ƒëe_d·ªça", "gh√™", "hi·ªÉm_√°c",
            "l·ª´a_d·ªëi", "l·ª´a", "m·∫∑n", "t·ªá_nh·∫•t", "b·∫©n_th·ªâu", "h√†_kh·∫Øc",
            "cay", "ngu_d·ªët", "hi·∫øm", "ng∆∞·ª£c_ƒë√£i", "ch·∫≠m", "cƒÉng_th·∫≥ng",
            "th√¥_b·∫°o", "kh√≥_ch·ªãu", "kh·∫Øc_nghi·ªát", "k·ªã", "ghen", "h·ªón_t·∫°p",
            "d∆°", "li·ªÅu_lƒ©nh", "d∆°_b·∫©n", "th√¥_t·ª•c", "t·ªá_h·∫°i", "t·ªá",
            "nh·∫ßm_l·∫´n", "qu√°_m·ª©c", "x·∫•u_s·ªë", "ngu_si", "ƒëau_ƒë·ªõn", "ph√†n_n√†n",
            "ph·∫£n_c·∫£m", "t√†n_ph√°", "b·∫•t_m√£n", "hung_hƒÉng", "b·∫•t_ti·ªán",
            "hoang_s∆°", "gi·∫£_d·ªëi", "ƒë·∫Øt_ƒë·ªè", "ƒë·∫Øt", "y·∫øu", "sai_l·∫ßm", "l·∫ßm",
            "nghi√™m_tr·ªçng", "ƒë√°ng_gh√©t", "h·ªèng", "b·∫•t_h·ª£p_t√°c", "ch√°n_n·∫£n",
            "y·∫øu_ƒëu·ªëi", "tr·ª•c_tr·∫∑c", "b·ª±c_b·ªôi", "t√†n_b·∫°o", "b·ª´a_b√£i",
            "lƒÉng_nhƒÉng", "th·∫•t_v·ªçng", "ch√™_bai", "loang_l·ªï", "ti√™u_hao",
            "b·∫•t_c√¥ng", "lang_thang", "kh·ªï_s·ªü", "v·ªõ_v·∫©n", "b·∫•t_h·∫°nh",
            "v√¥_t√¢m", "b√π_x√π", "b·ª´a_b·ªôn", "kh√≥", "gian_d·ªëi", "v√¥_d·ª•ng",
            "v√¥_nghƒ©a", "√°c", "ch√≥ng_m·∫∑t", "l√†_l·∫°", "mi·ªÖn_c∆∞·ª°ng", "ngu_ng·ªëc",
            "d·ªã_·ª©ng", "co_c·ª©ng", "h·∫°i", "l·∫°m_d·ª•ng", "vu_kh·ªëng", "tai_h·∫°i",
            "t·ªìi", "x·∫£o_quy·ªát", "ƒëau_th∆∞∆°ng", "h·ªón_lo·∫°n", "nh·ª©c_nh·ªëi",
            "ƒë·ªè_ng·∫ßu", "lo√©t", "s∆∞ng_t·∫•y", "t·∫•y", "vi√™m", "·ªëm_y·∫øu", "kh√¥",
            "n·∫∑ng_b·ª•ng", "n·∫∑ng_n·ªÅ", "kh√†n_kh√†n", "d·ªã", "l·∫≠t", "v√¥_v·ªçng",
            "gian_l·∫≠n", "xu·ªëng_c·∫•p", "·ª©_ƒë·ªçng", "l·∫°nh_to√°t", "oi_·∫£", "s∆∞ng",
            "b·ªã_nh·ªçt", "c√≥_√°c_c·∫£m", "t√†n_nh·∫´n", "m√π_qu√°ng", "b·∫•t_th∆∞·ªùng",
            "b·∫•t_t√≠n", "gay_g·∫Øt", "m·∫•t_l√≤ng", "b·∫°c_b·∫Ωo", "th√¥", "th·∫•t_s√°ch",
            "qu√°i_ƒë·∫£n", "th√π_ƒë·ªãch", "x√∫c_ph·∫°m", "b·∫•t_tr·ªã", "run", "g√¢y_m√™",
            "c·∫°n_ki·ªát", "t√†n_t·∫≠t", "ƒë·ªãnh_m·ªánh", "h√¥i_th·ªëi", "m·ªëc", "h√¥i",
            "g·∫´y", "l·ªüm", "h·∫Øc", "d·ªèm", "gi·ªüm", "d·ªüm", "nh√≤e", "ch·∫øt", "m√≥p",
            "m√πi_th·ªëi", "th·ªëi", "r√†ng_bu·ªôc", "h∆∞_h·ªèng", "b·ªã", "h∆∞", "gi·∫£_m·∫°o",
            "gi·∫£_t·∫°o", "gi·∫£", "s·ª£_h√£i", "kh√≥_khƒÉn", "b·ªëc_m√πi", "d√£_man",
            "nham_hi·ªÉm", "tham_nh≈©ng", "x·∫•u_xa", "·ªß_r≈©", "th√¢m", "k√≠ch_·ª©ng",
            "h·ªùn_d·ªói", "b√¥i_nh·ªç", "t√°c_h·∫°i", "tinh_ngh·ªãch", "kh√≥_ti√™u",
            "thong_th·∫£", "nh√†n_nh√£", "tr∆°", "th·ªëi_r·ªØa", "ph√π_phi·∫øm",
            "ƒë·ªôc_quy·ªÅn", "do_d·ª±", "n·∫°n_nh√¢n", "r·∫Øc_r·ªëi", "sai", "ƒë·ªãnh_ki·∫øn",
            "bu·ªìn_b√£", "b·ª©t_r·ª©t", "m√πi", "b·∫°i_ho·∫°i", "gi·∫≠n_d·ªØ", "b√°o_ƒë·ªông",
            "ph·∫´n_n·ªô", "gh√©t", "k√™nh_ki·ªáu", "nh√†m_ch√°n", "bu·ªìn", "x√≥t_xa",
            "ƒëau_l√≤ng", "1star", "2star", "ng·∫Øn", "t·ªïn_th·∫•t", "b·ª©c_x√∫c",
            "t√†n_√°c", "√°c_hi·ªÉm", "r·ªüm", "tr√≥c", "√°m_s√°t", "l∆∞·ªùi", "v·ª•n",
            "g√£y", "h·ªëi_ti·∫øc", "ti√™u_c·ª±c", "ngu", "h·ªët_ho·∫£ng", "ƒë·ªÉu", "nh√°i",
            "ng·ª©a", "c√πi", "h√†ng_l√¥", "h√†ng_gi·∫£", "ph·ª©c_t·∫°p", "n√°t", "m·ªù",
            "ƒë∆°", "ng·ªèm", "l√¢u", "n·∫∑ng", "th·ªßng", "tr·∫ßy", "d√£o", "l·ªói",
            "k√©m", "l√πn", "b√πn", "thi·∫øu", "r√°ch", "ng·∫•y", "t·ªìi_t·ªá", "m·∫ª",
            "·∫©u", "c·∫©u_th·∫£", "l·ªôn", "·∫ø_·∫©m", "·∫ø", "s∆∞·ªõt", "t·ªën_pin",
            "n√≥ng_m√°y", "n√≥ng", "gi·∫≠t_lag"
        }
        
        sentiment_lexicon = {}
        
        # Try to load from bundled VietSentiWordnet file
        lexicon_path = self.sentiment_lexicon_path
        if not lexicon_path:
            # Check bundled file location
            bundled_path = Path(__file__).parent / "VietSentiWordnet_ver1.0.txt"
            if bundled_path.exists():
                lexicon_path = str(bundled_path)
        
        if lexicon_path and Path(lexicon_path).exists():
            try:
                with open(lexicon_path, "r", encoding="utf-8") as file:
                    header_skipped = False
                    for line in file:
                        if not header_skipped:
                            if "POS\tID\tPosScore\tNegScore\tSynsetTerms\tGloss" in line:
                                header_skipped = True
                            continue
                        parts = line.strip().split("\t")
                        if len(parts) >= 5:
                            word = parts[4]
                            pos_score = float(parts[2])
                            neg_score = float(parts[3])
                            word_clean = word.split('#')[0]
                            if pos_score > 0.5:
                                sentiment_lexicon[word_clean] = "positive"
                                positive_words.add(word_clean)
                            if neg_score > 0.5:
                                sentiment_lexicon[word_clean] = "negative"
                                negative_words.add(word_clean)
                print(f"‚úÖ Loaded {len(sentiment_lexicon)} words from VietSentiWordnet")
            except Exception as e:
                print(f"‚ö†Ô∏è Could not load sentiment lexicon: {e}")
        
        # Add predefined words to lexicon
        for word in positive_words:
            if word not in sentiment_lexicon:
                sentiment_lexicon[word] = "positive"
        for word in negative_words:
            if word not in sentiment_lexicon:
                sentiment_lexicon[word] = "negative"
        
        return not_words, positive_words, negative_words, sentiment_lexicon
    
    def handle_negation(self, text: str) -> str:
        """Handle negation patterns in text."""
        text = self.remove_punctuation(text)
        not_words, positive_words, negative_words, _ = self.sentiment_lexicons
        
        try:
            from pyvi import ViTokenizer
            text = ViTokenizer.tokenize(text)
        except ImportError:
            pass
        
        texts = text.split()
        len_text = len(texts)
        
        i = 0
        while i < len_text:
            cp_text = texts[i]
            if cp_text in not_words and i < len_text - 1:
                next_word = texts[i + 1]
                if next_word in positive_words:
                    texts[i] = 'NOTPOS'
                    texts[i + 1] = ''
                elif next_word in negative_words:
                    texts[i] = 'NOTNEG'
                    texts[i + 1] = ''
            i += 1
        
        return ' '.join(filter(None, texts))
    
    def clean_text(self, text: str) -> str:
        """Full text cleaning pipeline."""
        if not isinstance(text, str):
            return ""
        
        text = self.lowercase(text)
        text = self.process_emojis(text)
        text = self.remove_elongated_chars(text)
        text = self.normalize_unicode(text)
        text = self.normalize_sentiment_words(text)
        text = self.handle_negation(text)
        text = self.remove_punctuation(text)
        text = ' '.join(text.split())
        
        return text.strip()
    
    def tokenize(self, text: str) -> str:
        """Tokenize text using pyvi or VnCoreNLP."""
        if not text.strip():
            return ""
        
        if self.word_segmenter == "pyvi":
            try:
                from pyvi import ViTokenizer
                return ViTokenizer.tokenize(text)
            except ImportError:
                return text
        else:
            try:
                tokens = self.word_segmenter.tokenize(text)
                return ' '.join(sum(tokens, []))
            except Exception:
                return text
    
    def close(self):
        """Close VnCoreNLP and forcefully kill the Java subprocess."""
        if self._word_segmenter and self._word_segmenter != "pyvi":
            try:
                # First try the standard close method
                self._word_segmenter.close()
                print("‚úÖ VnCoreNLP close() called")
            except Exception as e:
                print(f"‚ö†Ô∏è Error during VnCoreNLP close(): {e}")
            
            # Forcefully kill the Java subprocess if it exists
            try:
                if hasattr(self._word_segmenter, '_process') and self._word_segmenter._process:
                    proc = self._word_segmenter._process
                    if proc.poll() is None:  # Still running
                        print(f"üî™ Forcefully killing VnCoreNLP Java process (PID: {proc.pid})...")
                        proc.terminate()
                        try:
                            proc.wait(timeout=5)
                            print("‚úÖ VnCoreNLP process terminated gracefully")
                        except:
                            # Force kill if terminate doesn't work
                            proc.kill()
                            proc.wait()
                            print("‚úÖ VnCoreNLP process killed forcefully")
            except Exception as e:
                print(f"‚ö†Ô∏è Error killing VnCoreNLP process: {e}")
            
            # Clear the reference
            self._word_segmenter = None
            print("‚úÖ VnCoreNLP resources released")