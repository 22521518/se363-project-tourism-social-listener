"""
Vietnamese text processor for ACSA.
Includes word segmentation, emoji processing, sentiment word normalization, etc.
"""
import re
import unicodedata
import string
from typing import Tuple, Set, Dict, Optional
from pathlib import Path

from .base import TextProcessor


class VietnameseTextProcessor(TextProcessor):
    """Vietnamese text processor with word segmentation."""
    
    VNCORENLP_URL = "https://github.com/vncorenlp/VnCoreNLP/archive/refs/tags/v1.2.zip"
    VNCORENLP_DIR = "VnCoreNLP-1.2"
    VNCORENLP_JAR = "VnCoreNLP-1.2.jar"
    
    def __init__(self, 
                 vncorenlp_path: Optional[str] = None,
                 sentiment_lexicon_path: Optional[str] = None):
        """
        Initialize Vietnamese text processor.
        
        Args:
            vncorenlp_path: Path to VnCoreNLP JAR file
            sentiment_lexicon_path: Path to VietSentiWordnet file
        """
        self.vncorenlp_path = vncorenlp_path
        self.sentiment_lexicon_path = sentiment_lexicon_path
        self._word_segmenter = None
        self._sentiment_lexicons = None
    
    def _download_vncorenlp(self, target_dir: Path) -> Optional[str]:
        """Download and extract VnCoreNLP if not present."""
        import urllib.request
        import zipfile
        import shutil
        
        target_dir.mkdir(parents=True, exist_ok=True)
        zip_path = target_dir / "vncorenlp.zip"
        extract_dir = target_dir / self.VNCORENLP_DIR
        jar_path = extract_dir / self.VNCORENLP_JAR
        
        if jar_path.exists():
            print(f"âœ… VnCoreNLP already exists: {jar_path}")
            return str(jar_path)
        
        try:
            print(f"ðŸ“¥ Downloading VnCoreNLP from {self.VNCORENLP_URL}...")
            urllib.request.urlretrieve(self.VNCORENLP_URL, zip_path)
            
            print(f"ðŸ“¦ Extracting to {target_dir}...")
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(target_dir)
            
            # The zip contains VnCoreNLP-1.2/VnCoreNLP-1.2/...
            inner_dir = target_dir / f"VnCoreNLP-{self.VNCORENLP_DIR.split('-')[1]}"
            if inner_dir.exists() and not extract_dir.exists():
                shutil.move(str(inner_dir), str(extract_dir))
            
            # Clean up
            zip_path.unlink()
            
            if jar_path.exists():
                print(f"âœ… VnCoreNLP downloaded successfully: {jar_path}")
                return str(jar_path)
            else:
                print(f"âš ï¸ JAR not found after extraction at {jar_path}")
                return None
                
        except Exception as e:
            print(f"âŒ Failed to download VnCoreNLP: {e}")
            return None
        
    @property
    def word_segmenter(self):
        """Lazy load word segmenter (pyvi fallback)."""
        if self._word_segmenter is None:
            vncorenlp_path = self.vncorenlp_path
            
            # Try to find or download VnCoreNLP
            if not vncorenlp_path:
                # Check common locations
                possible_paths = [
                    Path("/opt/airflow/VnCoreNLP-1.2/VnCoreNLP-1.2.jar"),
                    Path.home() / "VnCoreNLP-1.2" / "VnCoreNLP-1.2.jar",
                    Path(__file__).parent.parent / "VnCoreNLP-1.2" / "VnCoreNLP-1.2.jar",
                ]
                for p in possible_paths:
                    if p.exists():
                        vncorenlp_path = str(p)
                        break
            
            if vncorenlp_path:
                try:
                    from vncorenlp import VnCoreNLP
                    self._word_segmenter = VnCoreNLP(
                        vncorenlp_path,
                        annotators="wseg",
                        quiet=True
                    )
                    print(f"âœ… VnCoreNLP loaded from: {vncorenlp_path}")
                except Exception as e:
                    print(f"âš ï¸ Could not load VnCoreNLP: {e}")
                    print("Falling back to pyvi tokenizer")
                    self._word_segmenter = "pyvi"
            else:
                print("â„¹ï¸ VnCoreNLP not found, using pyvi tokenizer")
                self._word_segmenter = "pyvi"
                
        return self._word_segmenter
    
    @property
    def sentiment_lexicons(self) -> Tuple[Set, Set, Set, Dict]:
        """Lazy load sentiment lexicons."""
        if self._sentiment_lexicons is None:
            self._sentiment_lexicons = self._load_sentiment_lexicon()
        return self._sentiment_lexicons
    
    def lowercase(self, text: str) -> str:
        """Convert text to lowercase."""
        return text.lower()
    
    def remove_elongated_chars(self, text: str) -> str:
        """Remove elongated characters (e.g., Ä‘áº¹ppppp -> Ä‘áº¹p)."""
        pattern = rf"(\w)\1+"
        text = re.sub(pattern, r'\1', text)
        return text
    
    def normalize_unicode(self, text: str) -> str:
        """Normalize Unicode characters."""
        return unicodedata.normalize("NFC", text)
    
    def process_emojis(self, text: str) -> str:
        """Convert emojis to sentiment labels."""
        emojis_list = {
            "ðŸ‘¹": "negative", "ðŸ‘»": "positive", "ðŸ’ƒ": "positive", 'ðŸ¤™': 'positive ',
            'ðŸ‘': 'positive ', "ðŸ’„": "positive", "ðŸ’Ž": "positive", "ðŸ’©": "positive",
            "ðŸ˜•": "negative", "ðŸ˜±": "negative", "ðŸ˜¸": "positive", "ðŸ˜¾": "negative",
            "ðŸš«": "negative", "ðŸ¤¬": "negative", "ðŸ§š": "positive", "ðŸ§¡": "positive",
            'ðŸ¶': 'positive ', 'ðŸ‘Ž': 'negative ', 'ðŸ˜£': 'negative ', 'âœ¨': 'positive ',
            'â£': 'positive ', 'â˜€': 'positive ', 'â™¥': 'positive ', 'ðŸ¤©': 'positive ',
            'like': 'positive ', ':))': 'positive ', ':)': 'positive ',
            'he he': 'positive ', 'hehe': 'positive ', 'hihi': 'positive ',
            'haha': 'positive ', 'hjhj': 'positive ', ' lol ': 'negative ',
            ' cc ': 'negative ', 'huhu': 'negative ', '><': 'positive ',
            'ðŸ’Œ': 'positive ', 'ðŸ¥°': 'positive ', 'ðŸ™†': 'positive ', 'ðŸ˜…': 'negative ',
            'ðŸ¤’': 'negative ', 'ðŸ¤¨': 'negative ', 'ðŸ¤¦': 'negative ', 'ðŸ˜¬': 'negative ',
            'ðŸ”‹': 'positive ', 'ðŸ’”': 'negative ', 'ðŸ¤®': 'negative ', 'âœ‹': 'positive ',
            'ðŸ¤£': 'positive ', 'ðŸ–¤': 'positive ', 'ðŸ¤¤': 'positive ', ':(': 'negative ',
            'ðŸ˜¢': 'negative ', 'â¤': 'positive ', 'ðŸ˜': 'positive ', 'ðŸ˜˜': 'positive ',
            'ðŸ˜ª': 'negative ', 'ðŸ˜Š': 'positive ', '?': ' ? ', 'ðŸ˜': 'positive ',
            'ðŸ’–': 'positive ', 'ðŸ˜Ÿ': 'negative ', 'ðŸ˜­': 'negative ', 'ðŸ’¯': 'positive ',
            'ðŸ’—': 'positive ', 'â™¡': 'positive ', 'ðŸ’œ': 'positive ', 'ðŸ¤—': 'positive ',
            '^^': 'positive ', 'ðŸ˜¨': 'negative ', 'â˜º': 'positive ', 'ðŸ’‹': 'positive ',
            'ðŸ‘Œ': 'positive ', 'ðŸ˜–': 'negative ', 'ðŸ˜€': 'positive ', ':((': 'negative ',
            'ðŸ˜¡': 'negative ', 'ðŸ˜ ': 'negative ', 'ðŸ˜’': 'negative ', 'ðŸ™‚': 'positive ',
            'ðŸ˜': 'negative ', 'ðŸ˜': 'positive ', 'ðŸ˜„': 'positive ', 'ðŸ˜™': 'positive ',
            'ðŸ˜¤': 'negative ', 'ðŸ˜Ž': 'positive ', 'ðŸ˜†': 'positive ', 'ðŸ’š': 'positive ',
            'âœŒ': 'positive ', 'ðŸ’•': 'positive ', 'ðŸ˜ž': 'negative ', 'ðŸ˜“': 'negative ',
            'ï¸ðŸ†—ï¸': 'positive ', 'ðŸ˜‰': 'positive ', 'ðŸ˜‚': 'positive ', ':v': 'positive ',
            '=))': 'positive ', 'ðŸ˜‹': 'positive ', 'ðŸ’“': 'positive ', 'ðŸ˜': 'negative ',
            ':3': 'positive ', 'ðŸ˜«': 'negative ', 'ðŸ˜¥': 'negative ', 'ðŸ˜ƒ': 'positive ',
            'ðŸ˜Œ': ' ðŸ˜Œ ', 'ðŸ’›': 'positive ', 'ðŸ¤': 'positive ', 'ðŸŽˆ': 'positive ',
            'ðŸ˜—': 'positive ', 'ðŸ¤”': 'negative ', 'ðŸ˜‘': 'negative ', 'ðŸ”¥': 'negative ',
            'ðŸ™': 'negative ', 'ðŸ†—': 'positive ', 'ðŸ˜»': 'positive ', 'ðŸ’™': 'positive ',
            'ðŸ’Ÿ': 'positive ', 'ðŸ˜š': 'positive ', 'âŒ': 'negative ', 'ðŸ‘': 'positive ',
            ';)': 'positive ', '<3': 'positive ', 'ðŸŒ': 'positive ', 'ðŸŒ·': 'positive ',
            'ðŸŒ¸': 'positive ', 'ðŸŒº': 'positive ', 'ðŸŒ¼': 'positive ', 'ðŸ“': 'positive ',
            'ðŸ…': 'positive ', 'ðŸ¾': 'positive ', 'ðŸ‘‰': 'positive ', 'ðŸ’': 'positive ',
            'ðŸ’ž': 'positive ', 'ðŸ’¥': 'positive ', 'ðŸ’ª': 'positive ', 'ðŸ’°': 'positive ',
            'ðŸ˜‡': 'positive ', 'ðŸ˜›': 'positive ', 'ðŸ˜œ': 'positive ', 'ðŸ™ƒ': 'negative ',
            'ðŸ¤‘': 'positive ', 'ðŸ¤ª': 'positive ', 'â˜¹': 'negative ', 'ðŸ’€': 'negative ',
            'ðŸ˜”': 'negative ', 'ðŸ˜§': 'negative ', 'ðŸ˜©': 'negative ', 'ðŸ˜°': 'negative ',
            'ðŸ˜³': 'negative ', 'ðŸ˜µ': 'negative ', 'ðŸ˜¶': 'negative ', 'ðŸ™': 'negative ',
            'ðŸŽ‰': 'positive '
        }
        for emoji, label in emojis_list.items():
            text = text.replace(emoji, f'EMO{label.upper()}')
        text = ' '.join(text.split())
        return text
    
    def normalize_sentiment_words(self, text: str) -> str:
        """Normalize common Vietnamese sentiment words and abbreviations."""
        sentiment_word_map = {
            'Ã´ kÃªi': ' ok ', 'okie': ' ok ', ' o kÃª ': ' ok ',
            'okey': ' ok ', 'Ã´kÃª': ' ok ', 'oki': ' ok ', ' oke ': ' ok ',
            ' okay': ' ok ', 'okÃª': ' ok ',
            ' tks ': ' cÃ¡m Æ¡n ', 'thks': ' cÃ¡m Æ¡n ', 'thanks': ' cÃ¡m Æ¡n ',
            'ths': ' cÃ¡m Æ¡n ', 'thank': ' cÃ¡m Æ¡n ',
            'â­': 'star ', '*': 'star ', 'ðŸŒŸ': 'star ',
            'kg ': ' khÃ´ng ', 'not': ' khÃ´ng ', ' kg ': ' khÃ´ng ',
            '"k ': ' khÃ´ng ', ' kh ': ' khÃ´ng ', 'kÃ´': ' khÃ´ng ',
            'hok': ' khÃ´ng ', ' kp ': ' khÃ´ng pháº£i ', ' kÃ´ ': ' khÃ´ng ',
            '"ko ': ' khÃ´ng ', ' ko ': ' khÃ´ng ', ' k ': ' khÃ´ng ',
            'khong': ' khÃ´ng ', ' hok ': ' khÃ´ng ',
            'cute': ' dá»… thÆ°Æ¡ng ', ' vs ': ' vá»›i ', 'wa': ' quÃ¡ ',
            'wÃ¡': ' quÃ¡', 'j': ' gÃ¬ ', '"': ' ',
            ' sz ': ' cá»¡ ', 'size': ' cá»¡ ', ' Ä‘x ': ' Ä‘Æ°á»£c ',
            'dk': ' Ä‘Æ°á»£c ', 'dc': ' Ä‘Æ°á»£c ', 'Ä‘k': ' Ä‘Æ°á»£c ', 'Ä‘c': ' Ä‘Æ°á»£c ',
            'authentic': ' chuáº©n chÃ­nh hÃ£ng ', ' aut ': ' chuáº©n chÃ­nh hÃ£ng ',
            ' auth ': ' chuáº©n chÃ­nh hÃ£ng ', 'store': ' cá»­a hÃ ng ',
            'shop': ' cá»­a hÃ ng ', 'sp': ' sáº£n pháº©m ', 'gud': ' tá»‘t ',
            'god': ' tá»‘t ', 'wel done': ' tá»‘t ', 'good': ' tá»‘t ',
            'gÃºt': ' tá»‘t ', 'sáº¥u': ' xáº¥u ', 'gut': ' tá»‘t ', ' tot ': ' tá»‘t ',
            ' nice ': ' tá»‘t ', 'perfect': 'ráº¥t tá»‘t', 'bt': ' bÃ¬nh thÆ°á»ng ',
            'time': ' thá»i gian ', 'qÃ¡': ' quÃ¡ ', ' ship ': ' giao hÃ ng ',
            ' m ': ' mÃ¬nh ', ' mik ': ' mÃ¬nh ', 'á»ƒ': 'á»ƒ',
            'product': 'sáº£n pháº©m', 'quality': 'cháº¥t lÆ°á»£ng', 'chat': ' cháº¥t ',
            'excelent': 'hoÃ n háº£o', 'bad': 'tá»‡', 'fresh': ' tÆ°Æ¡i ', 'sad': ' tá»‡ ',
            'date': ' háº¡n sá»­ dá»¥ng ', 'hsd': ' háº¡n sá»­ dá»¥ng ',
            'quickly': ' nhanh ', 'quick': ' nhanh ', 'fast': ' nhanh ',
            'delivery': ' giao hÃ ng ', ' sÃ­p ': ' giao hÃ ng ',
            'beautiful': ' Ä‘áº¹p tuyá»‡t vá»i ', ' tl ': ' tráº£ lá»i ', ' r ': ' rá»“i ',
            ' shopE ': ' cá»­a hÃ ng ', ' order ': ' Ä‘áº·t hÃ ng ',
            'cháº¥t lg': ' cháº¥t lÆ°á»£ng ', ' sd ': ' sá»­ dá»¥ng ', ' dt ': ' Ä‘iá»‡n thoáº¡i ',
            ' nt ': ' nháº¯n tin ', 'sÃ i': ' xÃ i ', 'bjo': ' bao giá» ',
            'thick': ' thÃ­ch ', 'thik': ' thÃ­ch ', ' sop ': ' cá»­a hÃ ng ',
            ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': ' ráº¥t ',
            'quáº£ ng ': ' quáº£ng  ', 'dep': ' Ä‘áº¹p ', ' xau ': ' xáº¥u ',
            'delicious': ' ngon ', 'hÃ g': ' hÃ ng ', 'qá»§a': ' quáº£ ',
            'iu': ' yÃªu ', 'fake': ' giáº£ máº¡o ', 'trl': 'tráº£ lá»i',
            ' por ': ' tá»‡ ', ' poor ': ' tá»‡ ', 'ib': ' nháº¯n tin ',
            'rep': ' tráº£ lá»i ', 'fback': ' feedback ', 'fedback': ' feedback '
        }
        for word, replacement in sentiment_word_map.items():
            text = text.replace(word, replacement)
        return text
    
    def remove_punctuation(self, text: str) -> str:
        """Remove punctuation from text."""
        translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))
        return text.translate(translator)
    
    def _load_sentiment_lexicon(self) -> Tuple[Set, Set, Set, Dict]:
        """Load sentiment lexicon from VietSentiWordnet file or default sets."""
        not_words = {
            "khÃ´ng", 'khÃ´ng_há»', "cháº³ng", "chÆ°a", "khÃ´ng_pháº£i", "cháº£", "máº¥t",
            "thiáº¿u", "vÃ´", "Ä‘áº¿ch", "Ä‘Ã©o", "kÃ©m", "ná»", "not",
            "bá»›t", "khÃ´ng_bao_giá»",
        }
        
        positive_words = {
            "Æ°ng_Ã½", "Æ°ng", "ká»¹", "Ä‘Æ°á»£c", "Ã´_kÃª", "ok", "má»‹n", "á»•n", "xinh",
            "chÃºc_má»«ng", "háº¡nh_phÃºc", "sang", "oÃ¡ch", "khen", "á»•n_Ä‘á»‹nh",
            "cáº£m_Æ¡n", "cÃ¡m_Æ¡n", "chuáº©n", "hoÃ n_thiá»‡n", "cháº¯c_cháº¯n", "sáº¡ch_sáº½",
            "hÃ i_lÃ²ng", "cháº¥t_lÆ°á»£ng", "háº¥p_dáº«n", "vui_váº»", "nguyÃªn_cháº¥t",
            "thuáº­n_lá»£i", "cÃ³_lá»£i", "tÃ­ch_cá»±c", "khuyáº¿n_khÃ­ch", "tá»‘t_hÆ¡n",
            "vá»‹_tha", "sáº¯c", "bÃ©n", "thÃ­ch_há»£p", "quÃ½_bÃ¡u", "sÃ¢u_sáº¯c",
            "thá»‹nh_vÆ°á»£ng", "xinh_Ä‘áº¹p", "rá»±c_rá»¡", "trong_sÃ¡ng", "cháº¥p_nháº­n_Ä‘Æ°á»£c",
            "khÃ©o_lÃ©o", "nghá»‡_thuáº­t", "yÃªn_tÃ¢m", "uyá»ƒn_chuyá»ƒn", "sÃ´i_Ä‘á»™ng",
            "nhÃ¢n_Ä‘áº¡o", "thÃ¢n_máº­t", "thoáº£i_mÃ¡i", "Ä‘áº·c_biá»‡t", "toÃ n_diá»‡n",
            "hÃ²a_Ä‘á»“ng", "hÃ i_hÃ²a", "thuáº­n_tiá»‡n", "lá»‹ch_sá»±", "may_máº¯n", "may",
            "Ä‘oan_trang", "pháº¥n_cháº¥n", "sÃ nh_Ä‘iá»‡u", "sÃ¡ng_suá»‘t", "kÃ­n_Ä‘Ã¡o",
            "mÃ¡t_máº»", "láº¥p_lÃ¡nh", "danh_dá»±", "dá»…_dÃ ng", "say_mÃª", "nhiá»‡t_tÃ¬nh",
            "Ä‘áº¡o_Ä‘á»©c", "trung_thá»±c", "trung_thÃ nh", "chung_thá»§y", "ngon",
            "chu_Ä‘Ã¡o", "ngÄƒn_náº¯p", "lÃ nh_máº¡nh", "há»£p_vá»‡_sinh", "khÃ´n",
            "khen_ngá»£i", "quÃ½_giÃ¡", "khÃ¡ng_khuáº©n", "Ãªm_tai", "tinh_tÃºy",
            "du_dÆ°Æ¡ng", "bá»•_Ã­ch", "há»“ng_hÃ o", "khá»e_khoáº¯n", "khá»e_máº¡nh",
            "khá»e", "máº¡nh", "sÄƒn_cháº¯c", "sung_sá»©c", "máº¡nh_khá»e", "tráº»_trung",
            "Ä‘Ã¹a", "Ä‘á»_cao", "quáº£n_lÃ½", "cÃ¡nh_tay_pháº£i", "nháº­n_dáº¡ng_Ä‘Æ°á»£c",
            "hoÃ n_háº£o", "trá»n_váº¹n", "lÃ½_tÆ°á»Ÿng", "dá»…_an_á»§i", "Ä‘áº¹p", "duyÃªn_dÃ¡ng",
            "tuyá»‡t_vá»i", "Ä‘Ã¡ng_ngÆ°á»¡ng_má»™", "thÃº_vá»‹", "ngá»t_ngÃ o", "láº¡c_quan",
            "sinh_lá»£i", "chÃ­nh_Ä‘Ã¡ng", "khiÃªm_tá»‘n", "minh_máº«n", "uy_tÃ­n",
            "vinh_dá»±", "tháº³ng_tháº¯n", "báº£o_Ä‘áº£m", "mÃ u_má»¡", "dá»…_chá»‹u", "tÆ°Æ¡i",
            "cáº©n_tháº­n", "Ä‘Ãºng", "hiá»‡u_quáº£", "cute", "dá»…_thÆ°Æ¡ng", "phÃª", "xá»‹n",
            "sá»‹n", "vui_tÃ­nh", "chÃ­nh_hÃ£ng", "thá»±c_sá»±", "vinh_quang",
            "thÃ¡nh_thiá»‡n", "vui_tÆ°Æ¡i", "gá»£i_cáº£m", "cÃ¢n_Ä‘á»‘i", "chÃ¢n_thÃ nh",
            "thÃ nh_tháº¡o", "tinh_táº¿", "kiÃªn_cá»‘", "thÃ¢n_thiá»‡n", "thÃ­ch",
            "tá»a_sÃ¡ng", "ngÆ°á»¡ng_má»™", "phÃ¹_há»£p", "hy_vá»ng", "tá»‘t_Ä‘áº¹p", "tá»‘t",
            "giá»i_giang", "lÃ´i_cuá»‘n", "uyÃªn_bÃ¡c", "yÃªu", "thÃ­ch_thÃº", "Ã¡i_Ã¢n",
            "chÃ¢n_tÃ¬nh", "chÄƒm_chÃºt", "tuyá»‡t", "nháº¹_nhÃµm", "xinh_xáº¯n", "giá»i",
            "khá»§ng", "Ä‘áº¡t", "há»£p_lÃ½", "há»£p_lÃ­", "sáº¡ch", "áº¥m", "má»m",
            "cáº£i_thiá»‡n", "tiá»‡n", "gá»n", "tin_tÆ°á»Ÿng", "nháº¡y", "nháº¡y_bÃ©n",
            "pin_ráº¥t_trÃ¢u", "bao_mÆ°á»£t", "pin_trÃ¢u", "sáº¡c_nhanh"
        }
        
        negative_words = {
            "báº¥t_lá»£i", "chÃ¡n", "cháº­t_háº¹p", "cháº­t", "tá»©c_giáº­n", "xáº¥u",
            "khá»§ng_khiáº¿p", "má»ng", "nháº§m", "Ä‘e_dá»a", "ghÃª", "hiá»ƒm_Ã¡c",
            "lá»«a_dá»‘i", "lá»«a", "máº·n", "tá»‡_nháº¥t", "báº©n_thá»‰u", "hÃ _kháº¯c",
            "cay", "ngu_dá»‘t", "hiáº¿m", "ngÆ°á»£c_Ä‘Ã£i", "cháº­m", "cÄƒng_tháº³ng",
            "thÃ´_báº¡o", "khÃ³_chá»‹u", "kháº¯c_nghiá»‡t", "ká»‹", "ghen", "há»—n_táº¡p",
            "dÆ¡", "liá»u_lÄ©nh", "dÆ¡_báº©n", "thÃ´_tá»¥c", "tá»‡_háº¡i", "tá»‡",
            "nháº§m_láº«n", "quÃ¡_má»©c", "xáº¥u_sá»‘", "ngu_si", "Ä‘au_Ä‘á»›n", "phÃ n_nÃ n",
            "pháº£n_cáº£m", "tÃ n_phÃ¡", "báº¥t_mÃ£n", "hung_hÄƒng", "báº¥t_tiá»‡n",
            "hoang_sÆ¡", "giáº£_dá»‘i", "Ä‘áº¯t_Ä‘á»", "Ä‘áº¯t", "yáº¿u", "sai_láº§m", "láº§m",
            "nghiÃªm_trá»ng", "Ä‘Ã¡ng_ghÃ©t", "há»ng", "báº¥t_há»£p_tÃ¡c", "chÃ¡n_náº£n",
            "yáº¿u_Ä‘uá»‘i", "trá»¥c_tráº·c", "bá»±c_bá»™i", "tÃ n_báº¡o", "bá»«a_bÃ£i",
            "lÄƒng_nhÄƒng", "tháº¥t_vá»ng", "chÃª_bai", "loang_lá»•", "tiÃªu_hao",
            "báº¥t_cÃ´ng", "lang_thang", "khá»•_sá»Ÿ", "vá»›_váº©n", "báº¥t_háº¡nh",
            "vÃ´_tÃ¢m", "bÃ¹_xÃ¹", "bá»«a_bá»™n", "khÃ³", "gian_dá»‘i", "vÃ´_dá»¥ng",
            "vÃ´_nghÄ©a", "Ã¡c", "chÃ³ng_máº·t", "lÃ _láº¡", "miá»…n_cÆ°á»¡ng", "ngu_ngá»‘c",
            "dá»‹_á»©ng", "co_cá»©ng", "háº¡i", "láº¡m_dá»¥ng", "vu_khá»‘ng", "tai_háº¡i",
            "tá»“i", "xáº£o_quyá»‡t", "Ä‘au_thÆ°Æ¡ng", "há»—n_loáº¡n", "nhá»©c_nhá»‘i",
            "Ä‘á»_ngáº§u", "loÃ©t", "sÆ°ng_táº¥y", "táº¥y", "viÃªm", "á»‘m_yáº¿u", "khÃ´",
            "náº·ng_bá»¥ng", "náº·ng_ná»", "khÃ n_khÃ n", "dá»‹", "láº­t", "vÃ´_vá»ng",
            "gian_láº­n", "xuá»‘ng_cáº¥p", "á»©_Ä‘á»ng", "láº¡nh_toÃ¡t", "oi_áº£", "sÆ°ng",
            "bá»‹_nhá»t", "cÃ³_Ã¡c_cáº£m", "tÃ n_nháº«n", "mÃ¹_quÃ¡ng", "báº¥t_thÆ°á»ng",
            "báº¥t_tÃ­n", "gay_gáº¯t", "máº¥t_lÃ²ng", "báº¡c_báº½o", "thÃ´", "tháº¥t_sÃ¡ch",
            "quÃ¡i_Ä‘áº£n", "thÃ¹_Ä‘á»‹ch", "xÃºc_pháº¡m", "báº¥t_trá»‹", "run", "gÃ¢y_mÃª",
            "cáº¡n_kiá»‡t", "tÃ n_táº­t", "Ä‘á»‹nh_má»‡nh", "hÃ´i_thá»‘i", "má»‘c", "hÃ´i",
            "gáº«y", "lá»Ÿm", "háº¯c", "dá»m", "giá»Ÿm", "dá»Ÿm", "nhÃ²e", "cháº¿t", "mÃ³p",
            "mÃ¹i_thá»‘i", "thá»‘i", "rÃ ng_buá»™c", "hÆ°_há»ng", "bá»‹", "hÆ°", "giáº£_máº¡o",
            "giáº£_táº¡o", "giáº£", "sá»£_hÃ£i", "khÃ³_khÄƒn", "bá»‘c_mÃ¹i", "dÃ£_man",
            "nham_hiá»ƒm", "tham_nhÅ©ng", "xáº¥u_xa", "á»§_rÅ©", "thÃ¢m", "kÃ­ch_á»©ng",
            "há»n_dá»—i", "bÃ´i_nhá»", "tÃ¡c_háº¡i", "tinh_nghá»‹ch", "khÃ³_tiÃªu",
            "thong_tháº£", "nhÃ n_nhÃ£", "trÆ¡", "thá»‘i_rá»¯a", "phÃ¹_phiáº¿m",
            "Ä‘á»™c_quyá»n", "do_dá»±", "náº¡n_nhÃ¢n", "ráº¯c_rá»‘i", "sai", "Ä‘á»‹nh_kiáº¿n",
            "buá»“n_bÃ£", "bá»©t_rá»©t", "mÃ¹i", "báº¡i_hoáº¡i", "giáº­n_dá»¯", "bÃ¡o_Ä‘á»™ng",
            "pháº«n_ná»™", "ghÃ©t", "kÃªnh_kiá»‡u", "nhÃ m_chÃ¡n", "buá»“n", "xÃ³t_xa",
            "Ä‘au_lÃ²ng", "1star", "2star", "ngáº¯n", "tá»•n_tháº¥t", "bá»©c_xÃºc",
            "tÃ n_Ã¡c", "Ã¡c_hiá»ƒm", "rá»Ÿm", "trÃ³c", "Ã¡m_sÃ¡t", "lÆ°á»i", "vá»¥n",
            "gÃ£y", "há»‘i_tiáº¿c", "tiÃªu_cá»±c", "ngu", "há»‘t_hoáº£ng", "Ä‘á»ƒu", "nhÃ¡i",
            "ngá»©a", "cÃ¹i", "hÃ ng_lÃ´", "hÃ ng_giáº£", "phá»©c_táº¡p", "nÃ¡t", "má»",
            "Ä‘Æ¡", "ngá»m", "lÃ¢u", "náº·ng", "thá»§ng", "tráº§y", "dÃ£o", "lá»—i",
            "kÃ©m", "lÃ¹n", "bÃ¹n", "thiáº¿u", "rÃ¡ch", "ngáº¥y", "tá»“i_tá»‡", "máº»",
            "áº©u", "cáº©u_tháº£", "lá»™n", "áº¿_áº©m", "áº¿", "sÆ°á»›t", "tá»‘n_pin",
            "nÃ³ng_mÃ¡y", "nÃ³ng", "giáº­t_lag"
        }
        
        sentiment_lexicon = {}
        
        # Try to load from bundled VietSentiWordnet file
        lexicon_path = self.sentiment_lexicon_path
        if not lexicon_path:
            # Check bundled file location
            bundled_path = Path(__file__).parent / "VietSentiWordnet_ver1.0.txt"
            if bundled_path.exists():
                lexicon_path = str(bundled_path)
        
        if lexicon_path and Path(lexicon_path).exists():
            try:
                with open(lexicon_path, "r", encoding="utf-8") as file:
                    header_skipped = False
                    for line in file:
                        if not header_skipped:
                            if "POS\tID\tPosScore\tNegScore\tSynsetTerms\tGloss" in line:
                                header_skipped = True
                            continue
                        parts = line.strip().split("\t")
                        if len(parts) >= 5:
                            word = parts[4]
                            pos_score = float(parts[2])
                            neg_score = float(parts[3])
                            word_clean = word.split('#')[0]
                            if pos_score > 0.5:
                                sentiment_lexicon[word_clean] = "positive"
                                positive_words.add(word_clean)
                            if neg_score > 0.5:
                                sentiment_lexicon[word_clean] = "negative"
                                negative_words.add(word_clean)
                print(f"âœ… Loaded {len(sentiment_lexicon)} words from VietSentiWordnet")
            except Exception as e:
                print(f"âš ï¸ Could not load sentiment lexicon: {e}")
        
        # Add predefined words to lexicon
        for word in positive_words:
            if word not in sentiment_lexicon:
                sentiment_lexicon[word] = "positive"
        for word in negative_words:
            if word not in sentiment_lexicon:
                sentiment_lexicon[word] = "negative"
        
        return not_words, positive_words, negative_words, sentiment_lexicon
    
    def handle_negation(self, text: str) -> str:
        """Handle negation patterns in text."""
        text = self.remove_punctuation(text)
        not_words, positive_words, negative_words, _ = self.sentiment_lexicons
        
        try:
            from pyvi import ViTokenizer
            text = ViTokenizer.tokenize(text)
        except ImportError:
            pass
        
        texts = text.split()
        len_text = len(texts)
        
        i = 0
        while i < len_text:
            cp_text = texts[i]
            if cp_text in not_words and i < len_text - 1:
                next_word = texts[i + 1]
                if next_word in positive_words:
                    texts[i] = 'NOTPOS'
                    texts[i + 1] = ''
                elif next_word in negative_words:
                    texts[i] = 'NOTNEG'
                    texts[i + 1] = ''
            i += 1
        
        return ' '.join(filter(None, texts))
    
    def clean_text(self, text: str) -> str:
        """Full text cleaning pipeline."""
        if not isinstance(text, str):
            return ""
        
        text = self.lowercase(text)
        text = self.process_emojis(text)
        text = self.remove_elongated_chars(text)
        text = self.normalize_unicode(text)
        text = self.normalize_sentiment_words(text)
        text = self.handle_negation(text)
        text = self.remove_punctuation(text)
        text = ' '.join(text.split())
        
        return text.strip()
    
    def tokenize(self, text: str) -> str:
        """Tokenize text using pyvi or VnCoreNLP."""
        if not text.strip():
            return ""
        
        if self.word_segmenter == "pyvi":
            try:
                from pyvi import ViTokenizer
                return ViTokenizer.tokenize(text)
            except ImportError:
                return text
        else:
            try:
                tokens = self.word_segmenter.tokenize(text)
                return ' '.join(sum(tokens, []))
            except Exception:
                return text
    
    def close(self):
        """Close VnCoreNLP if initialized."""
        if self._word_segmenter and self._word_segmenter != "pyvi":
            try:
                self._word_segmenter.close()
            except:
                pass